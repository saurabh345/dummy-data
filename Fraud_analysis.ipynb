{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67c2bb4-1384-4fa5-9108-db25270c39b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (2.3.1)\n",
      "Collecting panda\n",
      "  Downloading panda-0.3.1.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: setuptools in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from panda) (78.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from panda) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from requests->panda) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from requests->panda) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from requests->panda) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rising phoenix\\anaconda3\\envs\\pyspark_venv\\lib\\site-packages (from requests->panda) (2025.6.15)\n",
      "Building wheels for collected packages: panda\n",
      "  Building wheel for panda (setup.py): started\n",
      "  Building wheel for panda (setup.py): finished with status 'done'\n",
      "  Created wheel for panda: filename=panda-0.3.1-py3-none-any.whl size=7296 sha256=a527e01aa05e319bdd1a0fcfae3f07ead2458574dfeb465f33a8dec9f866e7b4\n",
      "  Stored in directory: c:\\users\\rising phoenix\\appdata\\local\\pip\\cache\\wheels\\df\\5c\\39\\36f8dae25a1e88d6ec4411dec4a143781e64fdff6897758eec\n",
      "Successfully built panda\n",
      "Installing collected packages: panda\n",
      "Successfully installed panda-0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'panda' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'panda'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install panda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b90deb4-afb7-4863-aec0-c51bf154332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, isnull, floor, skewness, kurtosis\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f199d70-503e-4005-bc0c-6b90f6c43a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Fraudscript\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c4e818-e346-4cd4-b206-76bbf1c98c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DATA ANALYSIS (15 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac987433-c39a-4f2e-a03f-8ddc305e2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.csv(\"data/diabetes.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840957de-8df0-4404-b49e-12c2b7ee22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema and basic stats\n",
    "df.printSchema()\n",
    "df.describe().show()\n",
    "print(\"Rows:\", df.count())\n",
    "print(\"Columns:\", len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573bd24-789f-4135-95ab-5d7f06d27d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null value check\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00cb412-74d3-464f-8186-d90de0c6ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts\n",
    "df.groupBy(\"target_column\").count().show()\n",
    "\n",
    "#df.groupBy(\"Outcome\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b58b5-bda6-4c76-941b-551b22270ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and kurtosis\n",
    "numeric_columns = [\n",
    "    'Pregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DiabetesPedigreeFunction',\n",
    "    'Age'\n",
    "]         \n",
    "df.select([skewness(c).alias(c + \"_skew\") for c in numeric_columns]).show()\n",
    "df.select([kurtosis(c).alias(c + \"_skew\") for c in numeric_columns]).show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import log1p  # needed when 1 < 0 > 1 skewness\n",
    "\n",
    "df = df.withColumn(\"log_Insulin\", log1p(\"Insulin\"))\n",
    "\n",
    "from pyspark.sql.functions import log1p #same for kurtosis <0 low =0 normal >3 high\n",
    "\n",
    "df = df.withColumn(\"log_Insulin\", log1p(\"Insulin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bc04a-66ea-4994-b46e-3045f1949e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Numeric columns\n",
    "numeric_cols = [\n",
    "    'Pregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DiabetesPedigreeFunction',\n",
    "    'Age'\n",
    "]\n",
    "\n",
    "# Assemble numeric columns into a features vector\n",
    "vec_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df_vector = vec_assembler.transform(df).select(\"features\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = Correlation.corr(df_vector, \"features\").head()[0]\n",
    "corr_array = correlation_matrix.toArray()  # Convert to numpy array\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_corr = pd.DataFrame(data=corr_array, columns=numeric_cols, index=numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18448266-0cba-47dd-856b-5db2fd9dfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"target_column\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df) # only needed if you dont have 0/1 value in target instead yes and no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef46f1-4777-4c2d-9ad0-6d237e6da274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a8f10-639a-4a74-b4aa-0569ad92b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50463543-ba8b-4f94-84f3-45aa09445ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64a2a9-274f-43c3-80cc-56b25245fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pregnancies',\n",
    "                          'Glucose',\n",
    "                          'BloodPressure',\n",
    "                          'SkinThickness',\n",
    "                          'Insulin',\n",
    "                          'BMI',\n",
    "                          'DiabetesPedigreeFunction',\n",
    "                          'Age'],\n",
    "                outputCol=\"features\")\n",
    "assembler.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efba1a-d82f-4a8c-91bb-ef7b59d65404",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_df = assembler.transform(df).select(\"features\", \"Outcome\")\n",
    "model_input_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c47cfc-c536-406a-b78a-a5566eaea8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = model_input_df.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984e902-5898-45f5-ac6f-6b57c0f6f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073bf54-a0d8-41bc-909d-420f3b69071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2dac8-6911-4678-a3f6-9ccdf6691061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train_data)\n",
    "lr_train_predictions = lr_model.transform(train_data)\n",
    "lr_test_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca3132-cf96-47d8-901f-13e0fe75b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c29c-12fa-4e18-9bf3-5976b716e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f86009-e07f-4765-9201-729e83266f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"Outcome\", featuresCol=\"features\", maxDepth=3)\n",
    "dt_model = dt.fit(train_data)\n",
    "dt_train_predictions = dt_model.transform(train_data)\n",
    "dt_test_predictions = dt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17225b1-dbbd-4f1f-b5e4-16b17dfd1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Outcome\", featuresCol=\"features\", numTrees=20, maxDepth=5)\n",
    "rf_model = rf.fit(train_data)\n",
    "rf_train_predictions = rf_model.transform(train_data)\n",
    "rf_test_predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e40d3-59ac-4817-a9b5-6f4fa7c6a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vModel Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f256a-514c-4335-bfe5-948d61ce12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f017b-7920-490a-8274-6b47e6e2f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "evaluator_rec = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"Outcome\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d20c12-8461-4f9b-bbed-a172ee9d9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Metrics train -\n",
    "print(\"Decision Tree Train  - Accuracy:\", round(evaluator_acc.evaluate(dt_train_predictions)*100,2))\n",
    "print(\"Decision Tree Train- F1 Score:\", round(evaluator_f1.evaluate(dt_train_predictions)*100,2))\n",
    "print(\"Decision Tree Train- Precision:\", round(evaluator_prec.evaluate(dt_train_predictions)*100,2))\n",
    "print(\"Decision Tree Train- Recall:\", round(evaluator_rec.evaluate(dt_train_predictions)*100,2))\n",
    "dt_train_predictions.groupBy(\"outcome\", \"prediction\").count().show()\n",
    "\n",
    "print(\"Decision Tree Test- Accuracy:\", round(evaluator_acc.evaluate(dt_test_predictions)*100,2))\n",
    "print(\"Decision Tree Test - F1 Score:\", round(evaluator_f1.evaluate(dt_test_predictions)*100,2))\n",
    "print(\"Decision Tree Test- Precision:\", round(evaluator_prec.evaluate(dt_test_predictions)*100,2))\n",
    "print(\"Decision Tree Test- Recall:\", round(evaluator_rec.evaluate(dt_test_predictions)*100,2))\n",
    "dt_test_predictions.groupBy(\"outcome\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b26575-cfc1-44b1-b6c7-8a55bac601b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for Random forest make sure all value as needed\n",
    "\n",
    "print(\"RandomForest  Train  - Accuracy:\", round(evaluator_acc.evaluate(rf_train_predictions)*100,2))\n",
    "# print(\"RandomForest Train- F1 Score:\", round(evaluator_f1.evaluate(rf_train_predictions)*100,2))\n",
    "# print(\"RandomForest Train- Precision:\", round(evaluator_prec.evaluate(rf_train_predictions)*100,2))\n",
    "# print(\"RandomForest Train- Recall:\", round(evaluator_rec.evaluate(rf_train_predictions)*100,2))\n",
    "rf_train_predictions.groupBy(\"outcome\", \"prediction\").count().show() #fill test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f1d9a-34d8-4593-b41c-67497ebd2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Metrics - same for logistic\n",
    "print(\"Logistic Regression - Accuracy:\", evaluator_acc.evaluate(lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a04c24-f8c2-432d-958b-7bf41b67655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b737943-070f-4257-9229-b7771dfcedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f4abe-e5cb-404e-ba00-9cc67cbef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "\n",
    "#parameter grid for tuning\n",
    "dt_grid = ParamGridBuilder().addGrid(dt.maxDepth, range(4, 10)).build()\n",
    "\n",
    "#evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n",
    "\n",
    "#Set up CrossValidator\n",
    "dt_cv = CrossValidator(estimator=dt, estimatorParamMaps=dt_grid, evaluator=evaluator, numFolds=5, seed=50 )\n",
    "\n",
    "#Fit model with cross-validation\n",
    "dt_cv_model = dt_cv.fit(train_data)\n",
    "\n",
    "#Get best model and print best params\n",
    "\n",
    "best_dt_model = dt_cv_model.bestModel\n",
    "print(\"Best Decision Tree Parameters:\")\n",
    "print(\"maxDepth:\", best_dt_model.getOrDefault(\"maxDepth\"))\n",
    "\n",
    "#Predict and evaluate on test data\n",
    "dt_test_predictions = best_dt_model.transform(test_data)\n",
    "auc = evaluator.evaluate(dt_test_predictions)\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed6e1d-0fb9-4c03-9601-6831db8996fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest -- need to work on crossvalidation\n",
    "rf = RandomForestClassifier(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "\n",
    "rf_grid = (ParamGridBuilder()\n",
    "           .addGrid(rf.numTrees, [50, 100])\n",
    "           .addGrid(rf.maxDepth, [2, 3, 4, 5, 10])\n",
    "           .build())\n",
    "\n",
    "rf_cv = CrossValidator(estimator=rf,\n",
    "                       estimatorParamMaps=rf_grid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=3)\n",
    "\n",
    "rf_cv_model = rf_cv.fit(train_data)\n",
    "rf_best_model = rf_cv_model.bestModel\n",
    "\n",
    "rf_cv_predictions = rf_cv_model.transform(test_data)\n",
    "\n",
    "best_rf_model = rf_cv_model.bestModel\n",
    "\n",
    "print(\"  Best Random Forest Parameters:\")\n",
    "print(\"  numTrees:\", best_rf_model.getOrDefault(\"numTrees\"))\n",
    "print(\"  maxDepth:\", best_rf_model.getOrDefault(\"maxDepth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d56da-4440-41d4-8de7-07eb0cb18253",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5])\\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "cv_model = cv.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "print(\"Test AUC:\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4834a-cfd8-4ea1-bc7f-e3e22b978043",
   "metadata": {},
   "outputs": [],
   "source": [
    "map reduce - \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
